<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="CURE: Learning Cross-Video Neural Representations for High-Quality Frame Interpolation">
    <meta name="author" content="Wentao Shangguan,
                                Yu Sun,
                                Weijie Gan,
                                Ulugbek S. Kamilov">

    <title>Learning Cross-Video Neural Representations for High-Quality Frame Interpolation</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
<!--    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>-->
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>CURE: Learning Cross-Video Neural Representations </br>for High-Quality Frame Interpolation</h2>
            <p class="abstract">The first Neural Field (NF) based Video Frame Interpolation (VFI) Algorithm.</p>
    <hr>
    <p class="authors">
        <a href="https://orcid.org/0000-0002-5784-5093"> Wentao Shangguan</a>,
        <a href="https://sunyumark.github.io/"> Yu Sun</a>,
        <a href="https://wjgancn.github.io/"> Weijie Gan</a>,
        <a href="https://engineering.wustl.edu/faculty/Ulugbek-Kamilov.html"> Ulugbek S. Kamilov</a> </br>
        <b><a href="https://cigroup.wustl.edu/"> Computational Imaging Group (CIG)</a></b>
    </p>

    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/">Paper</a>
        <a class="btn btn-primary" href="https://www.youtube.com/watch?v=o0cMvpPYHZQ">Video</a>
        <a class="btn btn-primary" href="https://github.com/WtSg/CURE">Code</a>
    </div>
</div>

<div class="container">
    <div class="section">
        <div class="vcontainer">
            <iframe class="video" src="https://www.youtube.com/embed/o0cMvpPYHZQ"
                    title="YouTube video player" frameborder="0"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                    allowfullscreen></iframe>
        </div><br>
        <h2>Abstract</h2>
        <hr>
        <p>
            This paper considers the problem of temporal video interpolation,
            where the goal is to synthesize a new video frame given its two
            neighbors. We propose <em><strong>C</strong>ross-Video
            Ne<strong>u</strong>ral <strong>R</strong>epres<strong>e</strong>ntation
            (<strong>CURE</strong>)</em> as the first video interpolation method
            based on <em>neural fields (NF)</em>. NF refers to the recent class of
            methods for neural representation of complex 3D scenes that has seen
            widespread success and application across computer vision. CURE
            represents the video as a continuous function parameterized by a
            coordinate-based neural network, whose inputs are the spatiotemporal
            coordinates and outputs are the corresponding RGB values.  introduces a
            new architecture that conditions the neural network on the input frames
            for imposing space-time consistency in the synthesized video. This not
            only improves the final interpolation quality, but also enables  to
            learn a prior across multiple videos. Experimental evaluations show that
             achieves the state-of-the-art performance on video interpolation on
            several benchmark datasets.
        </p>
    </div>

    <div class="section">
        <h2>Model</h2>
        <hr>
        <div class="row align-items-center">
			<div class="col justify-content-center text-center">
                <img src="img/scheme.png" >
                
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Representing images</h2>
        <hr>
        <p>
            A Siren that maps 2D pixel coordinates to a color may be used to parameterize images. Here, we supervise Siren
            directly with ground-truth pixel values. Siren not only fits the image with a 10 dB higher PSNR and in significantly
            fewer iterations than all baseline architectures, but is also the only MLP that accurately represents the first-
            and second order derivatives of the image.
        </p>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/image_convergence_15s_label.mp4" type="video/mp4">
                </video>
            </div> 
        </div>
        <div class="row align-items-center">
			<div class="col justify-content-center text-center">
                <video width="40%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/psnr_image_convergence_15s.mp4" type="video/mp4">
                </video>
            </div> 
        </div>
    </div>



    <div class="section">
        <h2>Representing Video</h2>
        <hr>
        <p>
            A Siren with pixel coordinates together with a time coordinate can be used to parameterize a video.
            Here, Siren is directly supervised with the ground-truth pixel values, and parameterizes video significantly
            better than a ReLU MLP.
        </p>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <video width="50%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/cat_comparison_label.mp4" type="video/mp4">
                </video>
            </div>
            <div class="col justify-content-center text-center">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/bikes_comparison_label.mp4" type="video/mp4">
                </video>
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Solving the Poisson Equation</h2>
        <hr>
        <p>
            By supervising only the derivatives of Siren, we can solve <a href="https://en.wikipedia.org/wiki/Poisson%27s_equation">Poisson's equation</a>.
            Siren is again the only architecture that fits image, gradient, and laplacian domains accurately and swiftly.
        </p>
         <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/poisson_convergence_15s_label.mp4" type="video/mp4">
                </video>
            </div> 
        </div>
        <div class="row align-items-center">
			<div class="col justify-content-center text-center">
                <video width="40%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/psnr_poisson_convergence_15s.mp4" type="video/mp4">
                </video>
            </div> 
        </div>
    </div>

    <div class="section">
        <h2>Representing shapes by solving the Eikonal equation<br>
            Interactive 3D SDF Viewer - Use Your Mouse to Navigate the Scenes</h2>
        <hr>
        <p>
            We can recover an SDF from a pointcloud and surface normals by solving the <a
                href="https://www.google.com/search?client=ubuntu&channel=fs&q=eikonal+equation&ie=utf-8&oe=utf-8">Eikonal
            equation</a>,
            a first-order boundary value problem. SIREN can recover a room-scale scene given only its pointcloud
            and surface normals, accurately reproducing fine detail, in less than an hour of training.
            In contrast to recent work on combining voxel grids with neural implicit representations,
            this stores the full scene in the weights of a single, 5-layer neural network, with no 2D or 3D
            convolutions, and orders of magnitude fewer parameters. Zoom in to compare fine detail!
            <b>Note that these SDFs are not supervised with ground-truth SDF / occupancy values, but rather, are the
            result of solving the above Eikonal boundary value problem. This is a significantly harder task,
            which requires supervision in the gradient domain (see paper). As a result, architectures whose gradients
            are not well-behaved perform worse than SIREN.</b>
        </p>
        <div class="container">
            <div class="row align-items-center">
                <div class="col-md-6 padding-0 canvas-row">
                    <h4>Room - Siren</h4>
                    <model-viewer
                            alt="Room Siren"
                            src="img/room_siren.glb"
                            style="width: 100%; height:300px; background-color: #404040"
                            exposure=".8"
                            camera-orbit="0deg 75deg 105%"
                            auto-rotate
                            camera-controls>
                    </model-viewer>
                </div>
                <div class="col-md-6 padding-0 canvas-row">
                    <h4>Room - ReLU</h4>
                    <model-viewer
                            alt="Room ReLU"
                            src="img/room_relu.glb"
                            style="width: 100%; height: 300px; background-color: #404040"
                            exposure=".8"
                            auto-rotate
                            camera-controls>
                    </model-viewer>
                    <!--                            poster="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5e7e6db2d75a9b467eee4111_legomesh_cover.png"-->
                </div>
            </div>
            <div class="row align-items-center">
                <div class="col-md-4 padding-0 canvas-row">
                    <h4>Statue - Siren</h4>
                    <model-viewer
                            alt="Statue Siren"
                            src="img/statue_siren.glb"
                            style="width: 100%; height: 600px; background-color: #404040"
                            exposure=".8"
                            camera-orbit="0deg 75deg 20%"
                            auto-rotate
                            camera-controls>
                    </model-viewer>
                </div>
                <div class="col-md-4 padding-0 canvas-row">
                    <h4>Statue - ReLU Pos. Enc.</h4>
                    <model-viewer
                            alt="Statue Positional Encoding"
                            src="img/statue_relu_pe.glb"
                            style="width: 100%; height: 600px; background-color: #404040"
                            exposure=".8"
                            camera-orbit="0deg 75deg 20%"
                            auto-rotate
                            camera-controls>
                    </model-viewer>
                </div>
                <div class="col-md-4 padding-0 canvas-row">
                    <h4>Statue - ReLU</h4>
                    <model-viewer
                            alt="Statue ReLU"
                            src="img/statue_relu.glb"
                            style="width: 100%; height: 600px; background-color: #404040"
                            exposure=".8"
                            camera-orbit="0deg 75deg 20%"
                            auto-rotate
                            camera-controls>
                    </model-viewer>
                </div>
            </div>
        </div>

        <!-- Loads <model-viewer> for modern browsers: -->
        <script type="module"
                src="https://unpkg.com/@google/model-viewer/dist/model-viewer.js">
        </script>
    </div>

    <div class="section">
        <h2>Solving the Helmholtz equation</h2>
        <hr>
        <p>
            Here, we use Siren to solve the <a href="https://en.wikipedia.org/wiki/Helmholtz_equation">inhomogeneous Helmholtz equation</a>.
            ReLU- and Tanh-based architectures fail entirely to converge to a solution.
        </p>
        <div class="gif">
            <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                <source src="img/helmholtz_convergence_video_pad_label.mp4" type="video/mp4">
            </video>
        </div>
    </div>

    <div class="section">
        <h2>Solving the wave equation</h2>
        <hr>
        <p>
            In the time domain, Siren succeeds to solve the wave equation, while a Tanh-based architecture fails to discover the
            correct solution.
        </p>
        <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="" class="gif">
            <source src="img/wave_combined_pad_label.mp4" type="video/mp4">
        </video>
    </div>

    <div class="section">
        <h2>Related Projects</h2>
        <hr>
        <p>
            Check out our related projects on the topic of implicit neural representations! <br>
        </p>
        <div class='row vspace-top'>
            <div class="col-sm-3">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/metasdf_steps_comp.mp4" type="video/mp4">
                </video>
            </div>

            <div class="col">
                <div class='paper-title'>
                    <a href="http://vsitzmann.github.io/metasdf/">MetaSDF: Meta-learning Signed Distance Functions</a>
                </div>
                <div>
                    We identify a key relationship between generalization across implicit neural representations and meta-
                    learning, and propose to leverage gradient-based meta-learning for learning priors over deep signed distance
                    functions. This allows us to reconstruct SDFs an order of magnitude faster than the auto-decoder framework,
                    with no loss in performance!
                </div>
            </div>
        </div>

        <div class='row vspace-top'>
            <div class="col-sm-3">
                <img src='img/SRNs.gif' class='img-fluid'>
            </div>

            <div class="col">
                <div class='paper-title'>
                    <a href="http://vsitzmann.github.io/srns/">Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations</a>

                </div>
                <div>
                    A continuous, 3D-structure-aware neural scene representation that encodes both geometry and appearance,
                    supervised only in 2D via a neural renderer, and generalizes for 3D reconstruction from a single posed 2D image.
                </div>
            </div>
        </div>

        <div class='row vspace-top'>
            <div class="col-sm-3">
                <img src='img/srn_seg_repimage.jpg' class='img-fluid'>
            </div>

            <div class="col">
                <div class='paper-title'>
                    <a href="https://www.computationalimaging.org/publications/semantic-srn/">Inferring Semantic Information with 3D Neural Scene Representations
                    </a>
                </div>
                <div>
                    We demonstrate that the features learned by neural implicit scene representations are useful for downstream
                    tasks, such as semantic segmentation, and propose a model that can learn to perform continuous 3D
                    semantic segmentation on a class of objects (such as chairs) given only a single, 2D (!) semantic label map!
                </div>
            </div>
        </div>

    <div class="section">
        <h2>Paper</h2>
        <hr>
        <div>
            <div class="list-group">
                <a href="https://arxiv.org/abs/2006.09661"
                   class="list-group-item">
                    <img src="img/paper_thumbnail.png" style="width:100%; margin-right:-20px; margin-top:-10px;">
                </a>
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Bibtex</h2>
        <hr>
        <div class="bibtexsection">
            @inproceedings{sitzmann2019siren,
                author = {Sitzmann, Vincent
                          and Martel, Julien N.P.
                          and Bergman, Alexander W.
                          and Lindell, David B.
                          and Wetzstein, Gordon},
                title = {Implicit Neural Representations
                          with Periodic Activation Functions},
                booktitle = {Proc. NeurIPS},
                year={2020}
            }
        </div>
    </div>

    <hr>

    <footer>
        <p>Send feedback and questions to <a href="http://web.stanford.edu/~sitzmann/">Vincent Sitzmann</a></p>
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>
